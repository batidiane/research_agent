{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Research Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a concept agent that is inspired by my process of researching on a subject. <br>\n",
    "I also took some inspiration from BabyAGI (without tools) implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"./.env\")\n",
    "import uuid\n",
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from yachalk import chalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedding_model = \"text-embedding-ada-002\"\n",
    "embeddings = OpenAIEmbeddings(model=text_embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the vector store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_localdb = True\n",
    "\n",
    "SUPABASE_PASSWORD = os.environ[\"SUPABASE_PASSWORD\"]\n",
    "SUPABASE_DBUSER = os.environ[\"SUPABASE_DBUSER\"]\n",
    "SUPABASE_DATABASE = os.environ[\"SUPABASE_DATABASE\"]\n",
    "supabasedb_string = f\"postgresql://{SUPABASE_DBUSER}:{SUPABASE_PASSWORD}@db.doxggeyqopdnxfhseufq.supabase.co:5432/{SUPABASE_DATABASE}\"\n",
    "\n",
    "PGVECTOR_USER = os.environ[\"PGVECTOR_USER\"]\n",
    "PGVECTOR_PASSWORD = os.environ[\"PGVECTOR_PASSWORD\"]\n",
    "PGVECTOR_DATABASE = os.environ[\"PGVECTOR_DATABASE\"]\n",
    "localdb_string = f\"postgresql://{PGVECTOR_USER}:{PGVECTOR_PASSWORD}@localhost:5432/{PGVECTOR_DATABASE}\"\n",
    "\n",
    "connection_string = localdb_string if use_localdb else supabasedb_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Law store\n",
    "from langchain.vectorstores import PGVector\n",
    "\n",
    "law_compilation_store = PGVector(\n",
    "    collection_name=\"law_compilation\",\n",
    "    connection_string=connection_string,\n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "\n",
    "## Mahabharata Store\n",
    "mahabharata_store = PGVector(\n",
    "    collection_name=\"mahabharat_combined_text\",\n",
    "    connection_string=connection_string,\n",
    "    embedding_function=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supabase vector store for storing runs\n",
    "\n",
    "The supabase client here is not used as a vector store.\n",
    "I am only using it to save the runs data.\n",
    "You can remove it if you dont need it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supabase.client import Client, create_client\n",
    "from langchain.vectorstores import SupabaseVectorStore\n",
    "\n",
    "supabase_url = os.environ.get(\"SUPABASE_URL\")\n",
    "supabase_key = os.environ.get(\"SUPABASE_SERVICE_KEY\")\n",
    "supabase: Client = create_client(supabase_url, supabase_key)\n",
    "\n",
    "runs_store = SupabaseVectorStore(\n",
    "    embedding=embeddings, client=supabase, table_name=\"runs\", query_name=\"match_runs\"\n",
    ")\n",
    "\n",
    "# ## Testing store\n",
    "# run_id = str(uuid.uuid4())\n",
    "# runs_store.add_texts(texts=[\"testing the store\"], metadatas=[{\"key\": \"value\"}], ids=[run_id])\n",
    "# matched_docs = runs_store.similarity_search_with_relevance_scores(\"testing store\", 1)\n",
    "# matched_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Chains\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question creator\n",
    "\n",
    "Generate new questions based on\n",
    "\n",
    "-   `question` - Original question. This is important so that the pertinence to the original question is always maintained. Or else the context can diverge quickly into impertinent results.\n",
    "-   `unanswered_questions`: So that the new questions do not overlap with the old ones.\n",
    "-   `context`: The answer to the last question.\n",
    "-   `num_questions`: Configurable hyper parameter.\n",
    "-   `start_id`: This is passed so that the ids of the newly generated question do overlap with the current list.\n",
    "\n",
    "---\n",
    "\n",
    "#### Most pertinent Question chain\n",
    "\n",
    "Pick the most pertinent question out of the given list of questions <br>\n",
    "I am not using any additional context other than the `original_question` for decising the pertinence.\n",
    "\n",
    "---\n",
    "\n",
    "#### Retrieval QA\n",
    "\n",
    "This chain is used to answer the intermediate questions. The idea is to generate succinct answers which can be used as notes to finally answer the original question\n",
    "\n",
    "---\n",
    "\n",
    "#### Result Analyser\n",
    "\n",
    "Not using this right now. I am not able to get this piece working well. So currently I will just run the agent for a fixed number of iterations and then compile the answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import all the chains.\n",
    "from chains_v2.create_questions import QuestionCreationChain\n",
    "from chains_v2.most_pertinent_question import MostPertinentQuestion\n",
    "from chains_v2.retrieval_qa import retrieval_qa\n",
    "from chains_v2.research_compiler import research_compiler\n",
    "from chains_v2.question_atomizer import QuestionAtomizer\n",
    "from chains_v2.refine_answer import RefineAnswer\n",
    "\n",
    "## Model with parameters\n",
    "gpt3t = \"gpt-3.5-turbo\"\n",
    "\n",
    "\n",
    "def language_model(\n",
    "    model_name: str = gpt3t, temperature: float = 0, verbose: bool = False\n",
    "):\n",
    "    llm = ChatOpenAI(model_name=model_name, temperature=temperature, verbose=verbose)\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Question helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.response_helpers import result2QuestionsList\n",
    "from helpers.response_helpers import qStr2Dict\n",
    "from helpers.questions_helper import getAnsweredQuestions\n",
    "from helpers.questions_helper import getUnansweredQuestions\n",
    "from helpers.questions_helper import getSubQuestions\n",
    "from helpers.questions_helper import getHopQuestions\n",
    "from helpers.questions_helper import getLastQuestionId\n",
    "from helpers.questions_helper import markAnswered\n",
    "from helpers.questions_helper import getQuestionById"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define log printers\n",
    "\n",
    "\n",
    "def print_iteration(current_iteration):\n",
    "    print(\n",
    "        chalk.bg_yellow_bright.black.bold(\n",
    "            f\"\\n   Iteration - {current_iteration}  â–·â–¶  \\n\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def print_unanswered_questions(unanswered):\n",
    "    print(\n",
    "        chalk.cyan_bright(\"** Unanswered Questions **\"),\n",
    "        chalk.cyan(\"\".join([f\"\\n'{q['id']}. {q['question']}'\" for q in unanswered])),\n",
    "    )\n",
    "\n",
    "\n",
    "def print_next_question(current_question_id, current_question):\n",
    "    print(\n",
    "        chalk.magenta.bold(\"** ðŸ¤” Next Questions I must ask: **\\n\"),\n",
    "        chalk.magenta(current_question_id),\n",
    "        chalk.magenta(current_question[\"question\"]),\n",
    "    )\n",
    "\n",
    "\n",
    "def print_answer(current_question):\n",
    "    print(\n",
    "        chalk.yellow_bright.bold(\"** Answer **\\n\"),\n",
    "        chalk.yellow_bright(current_question[\"answer\"]),\n",
    "    )\n",
    "\n",
    "\n",
    "def print_final_answer(answerpad):\n",
    "    print(\n",
    "        chalk.white(\"** Answer **\\n\"),\n",
    "        chalk.white(answerpad[-1]),\n",
    "    )\n",
    "\n",
    "\n",
    "def print_max_iterations():\n",
    "    print(\n",
    "        chalk.bg_yellow_bright.black.bold(\n",
    "            \"\\n âœ”âœ”  Max Iterations Reached. Compiling the results ...\\n\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def print_result(result):\n",
    "    print(chalk.italic.white_bright((result[\"text\"])))\n",
    "\n",
    "\n",
    "def print_sub_question(q):\n",
    "    print(chalk.magenta.bold(f\"** Sub Question **\\n{q['question']}\\n{q['answer']}\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Research Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question Data Schema\n",
    "\n",
    "```\n",
    "  Question: {\n",
    "    id: int,\n",
    "    question: string,\n",
    "    type: 'subquestion' | 'hops',\n",
    "    status: 'answered' | 'unanswered',\n",
    "    answer: string,\n",
    "    documents: []\n",
    "  }\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---- The researcher ----- ##\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    ## Create chains\n",
    "    def __init__(self, agent_settings, scratchpad, store, verbose):\n",
    "        self.store = store\n",
    "        self.scratchpad = scratchpad\n",
    "        self.agent_settings = agent_settings\n",
    "        self.verbose = verbose\n",
    "        self.question_creation_chain = QuestionCreationChain.from_llm(\n",
    "            language_model(\n",
    "                temperature=self.agent_settings[\"question_creation_temperature\"]\n",
    "            ),\n",
    "            verbose=self.verbose,\n",
    "        )\n",
    "        self.question_atomizer = QuestionAtomizer.from_llm(\n",
    "            llm=language_model(\n",
    "                temperature=self.agent_settings[\"question_atomizer_temperature\"]\n",
    "            ),\n",
    "            verbose=self.verbose,\n",
    "        )\n",
    "        self.most_pertinent_question = MostPertinentQuestion.from_llm(\n",
    "            language_model(\n",
    "                temperature=self.agent_settings[\"question_creation_temperature\"]\n",
    "            ),\n",
    "            verbose=self.verbose,\n",
    "        )\n",
    "        self.refine_answer = RefineAnswer.from_llm(\n",
    "            language_model(\n",
    "                temperature=self.agent_settings[\"refine_answer_temperature\"]\n",
    "            ),\n",
    "            verbose=self.verbose,\n",
    "        )\n",
    "\n",
    "    def run(self, question):\n",
    "        ## Step 0. Prepare the initial set of questions\n",
    "        atomized_questions_response = self.question_atomizer.run(\n",
    "            question=question,\n",
    "            num_questions=self.agent_settings[\"num_atomistic_questions\"],\n",
    "        )\n",
    "\n",
    "        self.scratchpad[\"questions\"] += result2QuestionsList(\n",
    "            question_response=atomized_questions_response,\n",
    "            type=\"subquestion\",\n",
    "            status=\"unanswered\",\n",
    "        )\n",
    "\n",
    "        for q in self.scratchpad[\"questions\"]:\n",
    "            q[\"answer\"], q[\"documents\"] = retrieval_qa(\n",
    "                llm=language_model(\n",
    "                    temperature=self.agent_settings[\"qa_temperature\"],\n",
    "                    verbose=self.verbose,\n",
    "                ),\n",
    "                retriever=self.store.as_retriever(\n",
    "                    search_type=\"mmr\", search_kwargs={\"k\": 5, \"fetch_k\": 10}\n",
    "                ),\n",
    "                question=q[\"question\"],\n",
    "                answer_length=self.agent_settings[\"intermediate_answers_length\"],\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "            q[\"status\"] = \"answered\"\n",
    "            print_sub_question(q)\n",
    "\n",
    "        current_context = \"\".join(\n",
    "            f\"\\n{q['id']}. {q['question']}\\n{q['answer']}\\n\"\n",
    "            for q in self.scratchpad[\"questions\"]\n",
    "        )\n",
    "\n",
    "        current_iteration = 0\n",
    "\n",
    "        while True:\n",
    "            current_iteration += 1\n",
    "            print_iteration(current_iteration)\n",
    "\n",
    "            # STEP 1: create questions\n",
    "            start_id = getLastQuestionId(self.scratchpad[\"questions\"]) + 1\n",
    "            questions_response = self.question_creation_chain.run(\n",
    "                question=question,\n",
    "                context=current_context,\n",
    "                previous_questions=[\n",
    "                    \"\".join(f\"\\n{q['question']}\") for q in self.scratchpad[\"questions\"]\n",
    "                ],\n",
    "                num_questions=self.agent_settings[\"num_questions_per_iteration\"],\n",
    "                start_id=start_id,\n",
    "            )\n",
    "            self.scratchpad[\"questions\"] += result2QuestionsList(\n",
    "                question_response=questions_response,\n",
    "                type=\"hop\",\n",
    "                status=\"unanswered\",\n",
    "            )\n",
    "\n",
    "            # STEP 2: Choose question for current iteration\n",
    "            unanswered = getUnansweredQuestions(self.scratchpad[\"questions\"])\n",
    "            unanswered_questions_prompt = self.unanswered_questions_prompt(unanswered)\n",
    "            print_unanswered_questions(unanswered)\n",
    "            response = self.most_pertinent_question.run(\n",
    "                original_question=question,\n",
    "                unanswered_questions=unanswered_questions_prompt,\n",
    "            )\n",
    "            current_question_dict = qStr2Dict(question=response)\n",
    "            current_question_id = current_question_dict[\"id\"]\n",
    "            current_question = getQuestionById(\n",
    "                self.scratchpad[\"questions\"], current_question_id\n",
    "            )\n",
    "            print_next_question(current_question_id, current_question)\n",
    "\n",
    "            # STEP 3: Answer the question\n",
    "            current_question[\"answer\"], current_question[\"documents\"] = retrieval_qa(\n",
    "                llm=language_model(\n",
    "                    temperature=self.agent_settings[\"qa_temperature\"],\n",
    "                    verbose=self.verbose,\n",
    "                ),\n",
    "                retriever=self.store.as_retriever(\n",
    "                    search_type=\"mmr\", search_kwargs={\"k\": 5, \"fetch_k\": 10}\n",
    "                ),\n",
    "                question=current_question[\"question\"],\n",
    "                answer_length=self.agent_settings[\"intermediate_answers_length\"],\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "            markAnswered(self.scratchpad[\"questions\"], current_question_id)\n",
    "            print_answer(current_question)\n",
    "            current_context = current_question[\"answer\"]\n",
    "\n",
    "            ## STEP 4: refine the answer\n",
    "            refinement_context = current_question[\"question\"] + current_context\n",
    "            refine_answer = self.refine_answer.run(\n",
    "                question=question,\n",
    "                context=refinement_context,\n",
    "                answer=self.get_latest_answer(),\n",
    "            )\n",
    "            self.scratchpad[\"answerpad\"] += [refine_answer]\n",
    "            print_final_answer(self.scratchpad[\"answerpad\"])\n",
    "\n",
    "            if current_iteration > self.agent_settings[\"max_iterations\"]:\n",
    "                print_max_iterations()\n",
    "                break\n",
    "\n",
    "    def unanswered_questions_prompt(self, unanswered):\n",
    "        return (\n",
    "            \"[\" + \"\".join([f\"\\n{q['id']}. {q['question']}\" for q in unanswered]) + \"]\"\n",
    "        )\n",
    "\n",
    "    def notes_prompt(self, answered_questions):\n",
    "        return \"\".join(\n",
    "            [\n",
    "                f\"{{ Question: {q['question']}, Answer: {q['answer']} }}\"\n",
    "                for q in answered_questions\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def get_latest_answer(self):\n",
    "        answers = self.scratchpad[\"answerpad\"]\n",
    "        answer = answers[-1] if answers else \"\"\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = str(uuid.uuid4())\n",
    "\n",
    "scratchpad = {\n",
    "    \"questions\": [],  # list of type Question\n",
    "    \"answerpad\": [],\n",
    "}\n",
    "\n",
    "store = law_compilation_store\n",
    "\n",
    "agent_settings = {\n",
    "    \"max_iterations\": 3,\n",
    "    \"num_atomistic_questions\": 2,\n",
    "    \"num_questions_per_iteration\": 4,\n",
    "    \"question_atomizer_temperature\": 0,\n",
    "    \"question_creation_temperature\": 0.4,\n",
    "    \"question_prioritisation_temperature\": 0,\n",
    "    \"refine_answer_temperature\": 0,\n",
    "    \"qa_temperature\": 0,\n",
    "    \"analyser_temperature\": 0,\n",
    "    \"intermediate_answers_length\": 200,\n",
    "    \"answer_length\": 500,\n",
    "}\n",
    "\n",
    "agent = Agent(agent_settings, scratchpad, store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is a private placement?\"\n",
    "\n",
    "agent = Agent(agent_settings, scratchpad, store)\n",
    "agent.run(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenAI@3111",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
